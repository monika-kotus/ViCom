{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracting themes and expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# New import path for v1:\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"xxx\"  # Add OpenAI key\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "MAX_CHARS_PER_CHUNK = 4000\n",
    "\n",
    "def chunk_text(text, chunk_size=MAX_CHARS_PER_CHUNK):\n",
    "    \"\"\"\n",
    "    Splits text into smaller pieces so we don't exceed token limits.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def call_openai_chat(messages):\n",
    "    \"\"\"\n",
    "    Calls the new openai v1 library client:\n",
    "    client.chat.completions.create(...)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0.0,   # 0 for deterministic\n",
    "            max_tokens=1000     # Adjust if you want longer or shorter responses\n",
    "        )\n",
    "        # Return the text from the first choice\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_and_classify(text):\n",
    "    \"\"\"\n",
    "    1) Split the researcher's text into chunks.\n",
    "    2) For each chunk, request up to 20 keywords/phrases.\n",
    "    3) Combine them, then classify into \"Themes\" vs. \"Expertise\" in JSON.\n",
    "    \"\"\"\n",
    "    text_chunks = chunk_text(text)\n",
    "    all_keywords = set()\n",
    "\n",
    "    # Extract up to 40 keywords from each chunk\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        if not chunk.strip():\n",
    "            continue\n",
    "\n",
    "        prompt_chunk = f\"\"\"\n",
    "You are a specialized assistant with expertise in linguistic, cognitive, and social science research.\n",
    "Below is a text chunk from a researcher's bio/publications:\n",
    "\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\n",
    "Identify up to 40 specific and actionable keywords or phrases that represent:\n",
    "1. \"Research Themes\" - Domains/subdomains (e.g., semantics, cognitive science, hearing loss, primate behavior), Specific theories (e.g., iconicity, motor theory, prosodic prominence)\n",
    "2. \"Core Expertise\" - Research methods (EEG, MRI, production/perception experiments), Data analysis methods (Bayesian stats, linear mixed-effects models, etc.)\n",
    "\n",
    "Avoid overly generic terms like \"language,\" \"communication,\" \"research,\" \"education,\" or \"methodology.\"\n",
    "Focus on unique and impactful keywords that highlight this researcherâ€™s work.\n",
    "\n",
    "Respond with a semicolon-separated list of keywords. If no relevant content is found, return an empty list.\n",
    "\"\"\"\n",
    "        \n",
    "        messages_chunk = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_chunk}\n",
    "        ]\n",
    "        chunk_result = call_openai_chat(messages_chunk)\n",
    "\n",
    "        # chunk_result might be something like: \"Primate behavior; Cognition; EEG; ...\"\n",
    "        # Convert to a list\n",
    "        chunk_list = [k.strip() for k in chunk_result.split(\";\") if k.strip()]\n",
    "        for item in chunk_list:\n",
    "            all_keywords.add(item)\n",
    "\n",
    "    # If no keywords found at all\n",
    "    if not all_keywords:\n",
    "        return {\"Themes\": [], \"Expertise\": []}\n",
    "\n",
    "    # Combine them into one string\n",
    "    combined_keywords = \"; \".join(all_keywords)\n",
    "    classify_prompt = f\"\"\"\n",
    "We have this list of keywords/phrases:\n",
    "\\\"\\\"\\\"{combined_keywords}\\\"\\\"\\\"\n",
    "\n",
    "1. Separate them into two categories: \"Research Themes\" (broad domain areas) \n",
    "   and \"Core Expertise\" (methods, technical skills, or specific tools).\n",
    "2. Limit each category to between 1 and 30 items, based on relevance.\n",
    "3. Return the result in JSON format exactly, like:\n",
    "\n",
    "{{\n",
    "  \"Themes\": [\"Theme1\", \"Theme2\", ...],\n",
    "  \"Expertise\": [\"Expertise1\", \"Expertise2\", ...]\n",
    "}}\n",
    "\n",
    "If you cannot fill some category, return an empty array for it.\n",
    "\"\"\"\n",
    "    messages_classify = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": classify_prompt}\n",
    "    ]\n",
    "    classify_result = call_openai_chat(messages_classify).strip()\n",
    "\n",
    "    # Attempt to parse as JSON\n",
    "    try:\n",
    "        parsed = json.loads(classify_result)\n",
    "        themes = parsed.get(\"Themes\", [])\n",
    "        expertise = parsed.get(\"Expertise\", [])\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: GPT classification not in JSON format. Returning empty.\")\n",
    "        themes, expertise = [], []\n",
    "\n",
    "    return {\"Themes\": themes, \"Expertise\": expertise}\n",
    "\n",
    "def main():\n",
    "    # Load your CSV\n",
    "    input_csv = \"Data_clean/06. Processed_Researcher_Data.csv\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df[\"Text\"] = df[\"Text\"].fillna(\"\")\n",
    "\n",
    "    all_themes = []\n",
    "    all_expertise = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        full_name = row[\"Full Name\"]\n",
    "        text = str(row[\"Text\"])\n",
    "        print(f\"Processing {full_name}...\")\n",
    "\n",
    "        result_dict = extract_and_classify(text)\n",
    "        # Convert lists to semicolon-separated strings\n",
    "        themes_str = \"; \".join(result_dict[\"Themes\"])\n",
    "        expertise_str = \"; \".join(result_dict[\"Expertise\"])\n",
    "\n",
    "        all_themes.append(themes_str)\n",
    "        all_expertise.append(expertise_str)\n",
    "\n",
    "    df[\"Themes\"] = all_themes\n",
    "    df[\"Expertise\"] = all_expertise\n",
    "\n",
    "    output_csv = \"Data_clean/08.researchers_with_themes_expertise_openai.csv\"\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Done! Results saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
